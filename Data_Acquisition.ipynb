{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will walk you through acquiring the related TNG-100 data before further analysis can be done. \n",
    "\n",
    "Note that the code in ```Figures.ipynb``` will not run unless the relevant data has been downloaded, processed and stored in compatible formats. So this notebook must be run before the figures from the paper (https://arxiv.org/abs/2105.02234) can be generated.\n",
    "\n",
    "## 0. _Get API key_\n",
    "\n",
    "**Downloading TNG data requires an API key from the IllustrisTNG server.** If this your first time working with TNG data, visit https://www.tng-project.org/data/docs/api/ and click on _New User Registration_ to create a user account and request an API key. \n",
    "\n",
    "#### Once in possession of your **API key**, navigate to ```simulation_data.__init__``` and set the variable ```API = \"ThisIsMyAPIKeyForIllustrisTNG\"``` with your API key as a string. \n",
    "\n",
    "You do not need to repeat this step during later iterations, unless your API key changes, or the variable ```API``` is not set permanently.\n",
    "\n",
    "## 1. _Import custom functions_\n",
    "\n",
    "First, import the necessary functions. Make sure to provide the right local path to the custom module ```simulation_data```. This module is required for generating the figures from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_data import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from simulation_data.galaxies import GalaxyPopulation\n",
    "my_galaxy_population = GalaxyPopulation()\n",
    "from simulation_data.galaxies.galaxy import get_galaxy_particle_data, get_stellar_assembly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. _Download particle data_\n",
    "\n",
    "Now, download targeted particle data for galaxies only within the specified mass cut at the specified redshift. This cell downloads particle data for all $z=2$ galaxies within $10^{10.5} \\leq M_{*}/M_{\\odot} \\leq 10^{12}$. \n",
    "\n",
    "Remember to check the path specified in the function ```get_galaxy_particle_data``` in ```simulation_data.galaxies.galaxy``` and make sure it points to a valid local drive. Within the target drive, create a folder titled ```'redshift_'+str(redshift)+'_data``` before running this cell. Note that the ```get_stellar_assembly_data``` needs a pre-existing stellar assembly file to run. Generating Figures 4 and 5 in the ```Figures``` notebook partially depends on the stellar assembly files for $z=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "Downloading http://www.tng-project.org/api/TNG100-1/snapshots/z=2/subhalos/9921\n",
      "Downloading http://www.tng-project.org/api/TNG100-1/snapshots/z=2/subhalos/9923\n",
      "Downloading http://www.tng-project.org/api/TNG100-1/snapshots/z=2/subhalos/9924\n"
     ]
    }
   ],
   "source": [
    "redshift = 2\n",
    "#this initializes the values in simulation_data.galaxies.galaxy_population\n",
    "ids = my_galaxy_population.select_galaxies(redshift=redshift, mass_min=10.5, mass_max=12)\n",
    "print(len(ids))\n",
    "#this gets and saves the particle data for each galaxy in our selection\n",
    "for idx in ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)\n",
    "    #Download Stellar Assembly Files for the chosen redshift before attempting to get particle assembly data\n",
    "    #get_stellar_assembly_data(id=idx, redshift=redshift, populate_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. _Find Main Progenitor and Descendent IDs_\n",
    "\n",
    "We now get the ids for the progenitors and descendents of our population identified at $z=2$. This example finds the main progenitor for each galaxy at $z=3$ and the descendant at $z=1.5$. It saves the arrays of ids at each redshift in the file ```redshift_ids.hdf5```. This step may be time-consuming. The intermediate print statements are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z=3 [-1 -1 -1 ... -1 -1 -1]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://www.tng-project.org/api/TNG100-1/snapshots/12/subhalos/21353/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-864db70e9cf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prog_sfid'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# request the full subhalo details of the progenitor by following the sublink URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'related'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sublink_progenitor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'snap'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mz3_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\moira\\TNG work\\Pathak_2021\\simulation_data\\__init__.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(path, params)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# raise exception if response code is not HTTP SUCCESS (200)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content-type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'application/json'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\envs\\astro\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://www.tng-project.org/api/TNG100-1/snapshots/12/subhalos/21353/"
     ]
    }
   ],
   "source": [
    "z2_ids = ids\n",
    "z3_ids = np.array([-1]*(len(ids)))\n",
    "z1_5_ids = np.array([-1]*(len(ids)))\n",
    "\n",
    "\n",
    "#Finding the progenitors at z=3   \n",
    "count = 0\n",
    "print('z=3', z3_ids)\n",
    "for i, id in enumerate(z2_ids):\n",
    "    if z3_ids[i] == -1:\n",
    "        start_url = \"http://www.tng-project.org/api/TNG100-1/snapshots/33/subhalos/\" + str(id)\n",
    "        sub = get(start_url)  \n",
    "        while sub['prog_sfid'] != -1:\n",
    "            # request the full subhalo details of the progenitor by following the sublink URL\n",
    "            sub = get(sub['related']['sublink_progenitor'])\n",
    "            if sub['snap'] == 25:\n",
    "                z3_ids[i] = sub['id'] \n",
    "    count += 1\n",
    "    print(count)\n",
    "with h5py.File('redshift_ids.hdf5', 'a') as f:\n",
    "    d1 = f.create_dataset('z3_ids', data = z3_ids)\n",
    "    d2 = f.create_dataset('z2_ids', data = z2_ids)\n",
    "    \n",
    "#Finding the descendants at z=1.5\n",
    "count = 0\n",
    "print('z=1.5', z1_5_ids)\n",
    "for i, id in enumerate(z2_ids):\n",
    "    if z1_5_ids[i] == -1:\n",
    "        start_url = \"http://www.tng-project.org/api/TNG100-1/snapshots/33/subhalos/\" + str(id)\n",
    "        sub = get(start_url)   \n",
    "        while sub['desc_sfid'] != -1:\n",
    "            # request the full subhalo details of the progenitor by following the sublink URL\n",
    "            sub = get(sub['related']['sublink_descendant'])\n",
    "            if sub['snap'] == 40:\n",
    "                z1_5_ids[i] = sub['id']         \n",
    "    count += 1\n",
    "    print(count)\n",
    "with h5py.File('redshift_ids.hdf5', 'a') as f:\n",
    "    d3 = f.create_dataset('z1.5_ids', data = z1_5_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. _Download particle data for linked ids at different redshifts_\n",
    "\n",
    "For each new set of ids for progenitors and descendants, now repeat the steps from the second cell to get and save the particle data for each galaxy at the new redshift. \n",
    "\n",
    "Remember to create new folders for each redshift you look at. We do not add the stellar assembly data for these redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('redshift_ids.hdf5', 'r') as f:\n",
    "    z1_5_ids = f['z1.5_ids'][:]\n",
    "    z3_ids = f['z3_ids'][:]\n",
    "\n",
    "redshift = 1.5\n",
    "for idx in z1_5_ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)\n",
    "    \n",
    "redshift = 3\n",
    "for idx in z1_5_ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our section on downloading data. \n",
    "\n",
    "## 5. _Calculate halo properties from particle data_\n",
    "\n",
    "To speed up analysis, we now calculate and store some halo properties in a separate hdf5 file named ```'galaxy_population_data_'+str(self.redshift)+'.hdf5'```. Remember to finish running the above section before moving on to the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = 2\n",
    "#this initializes the values in simulation_data.galaxies.galaxy_population\n",
    "ids = my_galaxy_population.select_galaxies(redshift=redshift, mass_min=10.5, mass_max=12)\n",
    "\n",
    "#calculate halo properties and store calculated data\n",
    "my_galaxy_population.get_galaxy_population_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the directions above will help you download and process IllustrsTNG data on your local machine. You can move on to the ```Figures.ipynb``` notebook after the data has been stored in compatible formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some shortcuts\n",
    "\n",
    "Walking down merger trees can be time consuming. Stellar assembly files may be hard to come by/compile from scratch. So here are some shortcuts. \n",
    "\n",
    "## _Relevant post-processed data included_\n",
    "\n",
    "Post-processed data from TNG-100 that has been used to generate the figures in this paper are included in ```galaxy_population_data_2.hdf5``` and ```redshift_ids.hdf5```. Each array stores a halo property for each halo within our mass-cut at $z=2$, arranged by the order of ids in ```'ids'```. These properties can also be easily recalculated from raw TNG-100 data and stored in the same format by following the steps above.\n",
    "\n",
    "This data is stored in a format compatible with the code in ```Figures.ipynb```. However, individual halo files are not included. Follow the steps above to download particle data for galaxies directly from the IllustrisTNG public resease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
